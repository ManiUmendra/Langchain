{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(input):\n",
    "    return input + \" from function_1\"\n",
    "\n",
    "def function_2(input):\n",
    "    return input + \" to function_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Graph()\n",
    "\n",
    "workflow.add_node(\"node_1\",function_1)\n",
    "workflow.add_node(\"node_2\",function_2)\n",
    "\n",
    "workflow.add_edge('node_1','node_2')\n",
    "workflow.set_entry_point('node_1')\n",
    "workflow.set_finish_point('node_2')\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am going from function_1 to function_2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke('I am going')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: node_1\n",
      "I am going from function_1\n",
      "key: node_2\n",
      "I am going from function_1 to function_2\n"
     ]
    }
   ],
   "source": [
    "for output in app.stream(\"I am going\"):\n",
    "    for key,value in output.items():\n",
    "        print(\"key:\",key)\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM call using Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import LLAMA_MODEL\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Graph()\n",
    "\n",
    "def llm_function_1(input):\n",
    "    prompt = \"Your task is to get the only country name from the sentence: \"+ input\n",
    "    value = model.invoke(prompt)\n",
    "    return value.content\n",
    "\n",
    "def llm_function_2(input):\n",
    "    return \"Country name is \"+input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_node('node_1',llm_function_1)\n",
    "workflow.add_node('node_2',llm_function_2)\n",
    "\n",
    "workflow.add_edge('node_1','node_2')\n",
    "workflow.set_entry_point('node_1')\n",
    "workflow.set_finish_point('node_2')\n",
    "\n",
    "app=workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Country name is The country name is:\\n\\nIndia'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\"My favurite country is India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_function_1(state):\n",
    "    val = state['messages']\n",
    "    prompt = \"Your task is to get the only specific topic name from the sentence: \"+ val[0]\n",
    "    value = model.invoke(prompt)\n",
    "    state['messages'].append(value.content)\n",
    "    return state\n",
    "\n",
    "def rag_function_2(state):\n",
    "    retriever = FAISS.load_local('web_data',OllamaEmbeddings(model=LLAMA_MODEL),allow_dangerous_deserialization=True).as_retriever()\n",
    "    prompt = '''Your task is to give the answer from the context based on given question:\n",
    "            {context}\n",
    "\n",
    "            Question : {question}\n",
    "        '''\n",
    "    prompt = ChatPromptTemplate.from_template(prompt)\n",
    "    chain = (\n",
    "        {\"context\":retriever,\"question\":RunnablePassthrough()}\n",
    "        |prompt\n",
    "        |model\n",
    "    )\n",
    "\n",
    "    que = state['messages']\n",
    "    value =chain.invoke(que[0])\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = Graph()\n",
    "\n",
    "workflow.add_edge('node_1','node_2')\n",
    "workflow.add_node('node_1',rag_function_1)\n",
    "workflow.add_node('node_2',rag_function_2)\n",
    "\n",
    "\n",
    "workflow.set_entry_point('node_1')\n",
    "workflow.set_finish_point('node_2')\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: node_1\n",
      "Value: {'messages': ['What is langsmith?', 'The specific topic name mentioned in the sentence \"What is Langsmith?\" is:\\n\\n* Langsmith']}\n",
      "Key: node_2\n",
      "Value: content='Based on the given context, LangSmith is a platform for building production-grade LLM (Large Language Model) applications. This information can be found in the first and third documents provided. The description of LangSmith is \"LangSmith is a platform for building production-grade LLM applications.\"' additional_kwargs={} response_metadata={'model': 'llama3', 'created_at': '2024-12-21T05:14:13.1456728Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 443259248400, 'load_duration': 63968900, 'prompt_eval_count': 824, 'prompt_eval_duration': 400826446000, 'eval_count': 58, 'eval_duration': 42360245000} id='run-f0cef1aa-d22c-44ab-a4ce-85a362ba4004-0' usage_metadata={'input_tokens': 824, 'output_tokens': 58, 'total_tokens': 882}\n"
     ]
    }
   ],
   "source": [
    "input = {\"messages\":[\"What is langsmith?\"]}\n",
    "for output in app.stream(input):\n",
    "    for key,value in output.items():\n",
    "        print(\"Key:\",key)\n",
    "        print(\"Value:\",value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicSelectionParser(BaseModel):\n",
    "    Topic: str = Field(description=\"Selected Topic\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=TopicSelectionParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Your task is to get the only specific topic name from the sentence: {question}  {format_instruction}'''\n",
    "\n",
    "template = PromptTemplate(template=prompt,input_variables=['question'],partial_variables={\"format_instruction\":parser.get_format_instructions()})\n",
    "\n",
    "\n",
    "chain = template |model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "val =chain.invoke({'question':'What is langsmith?','format_instruction':parser.get_format_instructions()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TopicSelectionParser(Topic='langsmith')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional State Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel,Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputParser(BaseModel):\n",
    "    Topic: str = Field(description=\"Selected Topic\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=OutputParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    prompt = '''Your task is to get the only specific topic name from the sentence: {query}  {format_instruction}'''\n",
    "\n",
    "    prompt = PromptTemplate(template=prompt,input_variables=['query'],partial_variables={\"format_instruction\":parser.get_format_instructions()})\n",
    "    chain = prompt | model | parser\n",
    "    \n",
    "    message=state['messages']\n",
    "    query = message[-1]\n",
    "    print(\"queyr:\",query)\n",
    "    value = chain.invoke({'query':query,\"format_instruction\":parser.get_format_instructions()})\n",
    "    print(\"Value:\",value)\n",
    "    state['messages'].append(value.Topic)\n",
    "    print(\"state:\",state)\n",
    "    return state\n",
    "\n",
    "def function_2(state):\n",
    "    print(\"RAG\")\n",
    "    retriever = FAISS.load_local('web_data',OllamaEmbeddings(model=LLAMA_MODEL),allow_dangerous_deserialization=True).as_retriever()\n",
    "    prompt = '''Your task is to answer the question based on provided context:\n",
    "             {context}\n",
    "             \n",
    "             Question : {question}\n",
    "    '''\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt)\n",
    "    chain = (\n",
    "        {\"context\":retriever,\"question\":RunnablePassthrough()}\n",
    "        |prompt\n",
    "        | ChatOllama(model=LLAMA_MODEL)\n",
    "    )\n",
    "\n",
    "    que = state['messages']\n",
    "    print(\"QUE:\",que[0])\n",
    "    que = que[0]\n",
    "    value = chain.invoke(que)\n",
    "    return value\n",
    "\n",
    "def function_3(state):\n",
    "    prompt = '''Your task is to answer the following question: {question}'''\n",
    "    prompt = PromptTemplate(template=prompt)\n",
    "    chain=prompt | model\n",
    "    que = state['messages']\n",
    "    value =chain.invoke({'question':que[0]})\n",
    "    print(\"Answer LLM:\",value.content)\n",
    "    return value\n",
    "\n",
    "def router(state):\n",
    "    messages = state['messages']\n",
    "    messages = messages[-1]\n",
    "    print(\"Reouter:\",messages)\n",
    "    if 'Langsmith' in messages:\n",
    "        return \"RAG\"\n",
    "    else:\n",
    "        return \"LLM\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict,Annotated,Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages : Annotated[Sequence[BaseMessage],operator.add]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph,END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node('agent',function_1)\n",
    "workflow.add_node('RAG',function_2)\n",
    "workflow.add_node('LLM',function_3)\n",
    "\n",
    "workflow.set_entry_point('agent')\n",
    "workflow.add_conditional_edges('agent',router,{\n",
    "    \"LLM\":\"LLM\",\n",
    "    \"RAG\":\"RAG\"\n",
    "})\n",
    "\n",
    "workflow.add_edge('LLM',END)\n",
    "workflow.add_edge('RAG',END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queyr: what is Langsmith?\n",
      "Value: Topic='Langsmith'\n",
      "state: {'messages': ['what is Langsmith?', 'Langsmith']}\n",
      "Reouter: Langsmith\n",
      "RAG\n",
      "QUE: what is Langsmith?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': ['what is Langsmith?',\n",
       "  'Langsmith',\n",
       "  'what is Langsmith?',\n",
       "  'Langsmith']}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = {'messages':['what is Langsmith?']}\n",
    "app.invoke(state)\n",
    "# for output in app.stream():\n",
    "#     for key,value in output.items():\n",
    "#         print(\"Key:\",key)\n",
    "#         print('Value:',value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools Making and Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from config import LLAMA_MODEL\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a:int,b:int):\n",
    "    \"Multiply two numbers together\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"llama3.2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [multiply]\n",
    "model = model.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(\"What is value of 35 * 2 ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': '',\n",
       " 'tool_calls': [{'function': {'name': 'multiply',\n",
       "    'arguments': {'a': 35, 'b': 2}}}]}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata.get('message')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph,END\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply_tools(a:int,b:int)->int:\n",
    "    \"Multiply two numbers together\"\n",
    "    return a * b\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\")\n",
    "tools = [multiply_tools]\n",
    "model = model.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(state):\n",
    "    message = state.get('messages')\n",
    "    message = message[-1]\n",
    "    \n",
    "    response = model.invoke(message)\n",
    "    print(response)\n",
    "    state['messages'].append(response.response_metadata.get('message').get('tool_calls')[0])\n",
    "\n",
    "    return state\n",
    "\n",
    "def invoke_tools(state):\n",
    "    #print(\"State:\",state)\n",
    "    arg = state.get('messages')[1].get('function').get('arguments')\n",
    "    \n",
    "    val = {}\n",
    "    for key, value in arg.items():\n",
    "        val[key]=int(value)\n",
    "    \n",
    "    response = multiply_tools.invoke(val)\n",
    "    state['messages'].append({\"value\":response})\n",
    "    return {\"value\":[response]}\n",
    "\n",
    "def router(state):\n",
    "    message = state.get('messages')[1].get('function').get('name')\n",
    "    if message=='multiply_tools':\n",
    "        return 'tools'\n",
    "    else:\n",
    "        return 'else'\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node('agent',invoke_model)\n",
    "graph.add_node('tools',invoke_tools)\n",
    "\n",
    "\n",
    "graph.set_entry_point('agent')\n",
    "graph.add_conditional_edges('agent',router,{\"tools\":\"tools\",\"else\":END})\n",
    "graph.add_edge('tools',END)\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' response_metadata={'model': 'llama3.2', 'created_at': '2024-12-22T17:28:01.0214053Z', 'message': {'role': 'assistant', 'content': '', 'tool_calls': [{'function': {'name': 'multiply_tools', 'arguments': {'a': '35', 'b': '37'}}}]}, 'done_reason': 'stop', 'done': True, 'total_duration': 9833989400, 'load_duration': 130417000, 'prompt_eval_count': 168, 'prompt_eval_duration': 1882000000, 'eval_count': 23, 'eval_duration': 7818000000} id='run-e6a6d338-e8b4-4584-9652-222b1c4afc1d-0' tool_calls=[{'name': 'multiply_tools', 'args': {'a': '35', 'b': '37'}, 'id': '915c70c9-901e-493b-b03c-6a27f63b142a', 'type': 'tool_call'}] usage_metadata={'input_tokens': 168, 'output_tokens': 23, 'total_tokens': 191}\n"
     ]
    }
   ],
   "source": [
    "messages = {'messages':['what is 35+2 ?']}\n",
    "ans = app.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': ['what is 35+2 ?',\n",
       "  {'function': {'name': 'multiply_tools',\n",
       "    'arguments': {'a': '35', 'b': '37'}}},\n",
       "  'what is 35+2 ?',\n",
       "  {'function': {'name': 'multiply_tools',\n",
       "    'arguments': {'a': '35', 'b': '37'}}},\n",
       "  {'value': 1295}]}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=multiply_tools.invoke({'a':10,'b':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
